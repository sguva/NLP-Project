{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b9f581d",
   "metadata": {},
   "source": [
    "Intrinsic evaluation as described in section 5.1 of the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d62e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import re\n",
    "import csv\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk import ngrams\n",
    "from nltk.metrics import edit_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd77fefc",
   "metadata": {},
   "source": [
    "To run it on results from our experiments set the variable 'use_saved_results' to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e01eb2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_saved_results = True\n",
    "if use_saved_results:\n",
    "    data = pd.read_csv('saved_files_from_our_experiments/answer_generation_results.csv')\n",
    "\n",
    "else:\n",
    "    data = pd.read_csv('answer_generation_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e339755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = data[\"answer\"]\n",
    "generated_answer = data[\"generated_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee48f602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 78.8619999999995\n"
     ]
    }
   ],
   "source": [
    "def calculate_bleu_score(reference, candidate):\n",
    "    reference = [reference.split()]  # Convert reference sentence to a list of words\n",
    "    candidate = candidate.split()  # Convert candidate sentence to a list of words\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    bleu_score = sentence_bleu(reference, candidate)\n",
    "\n",
    "    return bleu_score\n",
    "\n",
    "bleu_scores = []\n",
    "    \n",
    "    \n",
    "for i in range(len(answer)):\n",
    "    score = calculate_bleu_score(answer[i], generated_answer[i])\n",
    "    bleu_scores.append(round(score,2))\n",
    "avg = sum(bleu_scores)/len(bleu_scores)\n",
    "print(\"BLEU Score:\",avg*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5514c685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge Score for n=2 precision: 84.93160000000316\n",
      "Rouge Score for n=2 recall: 84.48460000000327\n",
      "Rouge Score for n=2 f1: 84.70520000000228\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "def calculate_rouge_score(reference, hypothesis):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(hypothesis, reference)\n",
    "    return scores\n",
    "\n",
    "rouge_scores_precision = []\n",
    "rouge_scores_recall = []\n",
    "rouge_scores_f1 = []\n",
    "\n",
    "for i in range(len(answer)):\n",
    "    score = calculate_rouge_score(answer[i], generated_answer[i])\n",
    "    p = score[0]['rouge-2']['p']\n",
    "    r = score[0]['rouge-2']['r']\n",
    "    f1 = score[0]['rouge-2']['f']\n",
    "    \n",
    "    rouge_scores_precision.append(round(p,2))\n",
    "    rouge_scores_recall.append(round(r,2))\n",
    "    rouge_scores_f1.append(round(f1,2))\n",
    "    \n",
    "p_avg = sum(rouge_scores_precision)/len(rouge_scores_precision)\n",
    "r_avg = sum(rouge_scores_recall)/len(rouge_scores_recall)\n",
    "f1_avg = sum(rouge_scores_f1)/len(rouge_scores_f1)\n",
    "\n",
    "print(\"Rouge Score for n=2 precision:\",p_avg*100)\n",
    "print(\"Rouge Score for n=2 recall:\",r_avg*100)\n",
    "print(\"Rouge Score for n=2 f1:\",f1_avg*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9630c780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
